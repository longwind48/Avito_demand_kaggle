{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lightgbm Implementation Avito w/o target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "by: Traci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook builds off several public kernels on kaggle. \n",
    "\n",
    "Data can be downloaded from https://www.kaggle.com/c/avito-demand-prediction.\n",
    "\n",
    "More information can be found in readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      " ['aggregated_features.csv', 'aggregated_features_v3.csv', 'aggregated_features_v5.csv', 'periods_test.csv', 'periods_train.csv', 'target_encoded.csv', 'test.csv', 'train.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\longwind48\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\n",
    "#improvement using kernel from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm\n",
    "#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n",
    "#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\n",
    "#Forked From - https://www.kaggle.com/him4318/avito-lightgbm-with-ridge-feature-v-2-0\n",
    "\n",
    "import time\n",
    "notebookstart= time.time()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "random.seed(2018)\n",
    "print(\"Data:\\n\",os.listdir(\"data\"))\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "NFOLDS = 5\n",
    "SEED = 2018\n",
    "VALID = True\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n",
    "        if(seed_bool == True):\n",
    "            params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('\\nFold {}'.format(i))\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        # textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\n",
    "        #regex = re.compile(u'[^[:alpha:]]')\n",
    "        #textProc = regex.sub(\" \", textProc)\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"\n",
    "    \n",
    "    \n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Load Stage\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Load Stage\")\n",
    "training = pd.read_csv('data/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "traindex = training.index\n",
    "testing = pd.read_csv('data/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testing.index\n",
    "\n",
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "y = training.deal_probability.copy()\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Combine Train and Test\")\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Add in aggregated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "gp = pd.read_csv('data/aggregated_features.csv') \n",
    "df = df.merge(gp, on='user_id', how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "df = df.set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(df.price.mean(),inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "df[\"avg_days_up_user\"] = np.log(df[\"avg_days_up_user\"]+0.001)\n",
    "df[\"avg_days_up_user\"].fillna(-999,inplace=True)\n",
    "df[\"avg_times_up_user\"] = np.log(df[\"avg_times_up_user\"]+0.001)\n",
    "df[\"avg_times_up_user\"].fillna(-999,inplace=True)\n",
    "df[\"n_user_items\"] = np.log(df[\"n_user_items\"]+0.001)\n",
    "df[\"n_user_items\"].fillna(-999,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create Time Variables\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreate Time Variables\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Label encoding cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encode Variables\n",
      "Encoding : ['user_id', 'region', 'city', 'parent_category_name', 'category_name', 'user_type', 'image_top_1', 'param_1', 'param_2', 'param_3']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncode Variables\")\n",
    "categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\"]\n",
    "print(\"Encoding :\",categorical)\n",
    "\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col].fillna('Unknown')\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText Features\")\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
    "\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    #df[cols + '_num_char'] = df[cols].apply(lambda comment: len(str(comment)))\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    df[cols + '_num_letters'] = df[cols].apply(lambda comment: len(comment)) # Count number of Letters\n",
    "    df[cols + '_num_alphabets'] = df[cols].apply(lambda comment: (comment.count(r'[a-zA-Z]'))) # Count number of Alphabets\n",
    "    df[cols + '_num_alphanumeric'] = df[cols].apply(lambda comment: (comment.count(r'[A-Za-z0-9]'))) # Count number of AlphaNumeric\n",
    "    df[cols + '_num_digits'] = df[cols].apply(lambda comment: (comment.count('[0-9]'))) # Count number of Digits\n",
    "    \n",
    "# Extra Feature Engineering\n",
    "df['avg_len_words_title'] = df['title_num_letters'] / df['title_num_words']\n",
    "df['avg_len_words_desc'] = df['description_num_letters'] / df['description_num_words']\n",
    "df['title_desc_len_ratio'] = df['title_num_letters']/df['description_num_letters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>...</th>\n",
       "      <th>title_num_words</th>\n",
       "      <th>title_num_unique_words</th>\n",
       "      <th>title_words_vs_unique</th>\n",
       "      <th>title_num_letters</th>\n",
       "      <th>title_num_alphabets</th>\n",
       "      <th>title_num_alphanumeric</th>\n",
       "      <th>title_num_digits</th>\n",
       "      <th>avg_len_words_title</th>\n",
       "      <th>avg_len_words_desc</th>\n",
       "      <th>title_desc_len_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>884270</td>\n",
       "      <td>19</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>249</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>кокоби(кокон для сна)</td>\n",
       "      <td>кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.362069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>227908</td>\n",
       "      <td>17</td>\n",
       "      <td>1314</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>стойка для одежды</td>\n",
       "      <td>стойка для одежды, под вешалки. с бутика.</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>576261</td>\n",
       "      <td>16</td>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>philips bluray</td>\n",
       "      <td>в хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.823529</td>\n",
       "      <td>0.141414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>755087</td>\n",
       "      <td>21</td>\n",
       "      <td>950</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>автокресло</td>\n",
       "      <td>продам кресло от0-25кг</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>944363</td>\n",
       "      <td>4</td>\n",
       "      <td>318</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>124</td>\n",
       "      <td>46</td>\n",
       "      <td>ваз 2110, 2003</td>\n",
       "      <td>все вопросы по телефону.</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id  region  city  parent_category_name  category_name  \\\n",
       "item_id                                                                    \n",
       "b912c3c6a6ad   884270      19   462                     4             42   \n",
       "2dac0150717d   227908      17  1314                     2             22   \n",
       "ba83aefab5dc   576261      16  1290                     0              2   \n",
       "02996f1dd2ea   755087      21   950                     4             42   \n",
       "7c90be56d2ab   944363       4   318                     6              0   \n",
       "\n",
       "              param_1  param_2  param_3                  title  \\\n",
       "item_id                                                          \n",
       "b912c3c6a6ad      249      112     1217  кокоби(кокон для сна)   \n",
       "2dac0150717d      122      112     1217      стойка для одежды   \n",
       "ba83aefab5dc       84      112     1217         philips bluray   \n",
       "02996f1dd2ea       38      112     1217             автокресло   \n",
       "7c90be56d2ab      278      124       46         ваз 2110, 2003   \n",
       "\n",
       "                                                    description  \\\n",
       "item_id                                                           \n",
       "b912c3c6a6ad  кокон для сна малыша,пользовались меньше месяц...   \n",
       "2dac0150717d          стойка для одежды, под вешалки. с бутика.   \n",
       "ba83aefab5dc  в хорошем состоянии, домашний кинотеатр с blu ...   \n",
       "02996f1dd2ea                             продам кресло от0-25кг   \n",
       "7c90be56d2ab                           все вопросы по телефону.   \n",
       "\n",
       "                      ...           title_num_words  title_num_unique_words  \\\n",
       "item_id               ...                                                     \n",
       "b912c3c6a6ad          ...                         3                       3   \n",
       "2dac0150717d          ...                         3                       3   \n",
       "ba83aefab5dc          ...                         2                       2   \n",
       "02996f1dd2ea          ...                         1                       1   \n",
       "7c90be56d2ab          ...                         3                       3   \n",
       "\n",
       "              title_words_vs_unique  title_num_letters  title_num_alphabets  \\\n",
       "item_id                                                                       \n",
       "b912c3c6a6ad                  100.0                 21                    0   \n",
       "2dac0150717d                  100.0                 17                    0   \n",
       "ba83aefab5dc                  100.0                 14                    0   \n",
       "02996f1dd2ea                  100.0                 10                    0   \n",
       "7c90be56d2ab                  100.0                 14                    0   \n",
       "\n",
       "              title_num_alphanumeric  title_num_digits  avg_len_words_title  \\\n",
       "item_id                                                                       \n",
       "b912c3c6a6ad                       0                 0             7.000000   \n",
       "2dac0150717d                       0                 0             5.666667   \n",
       "ba83aefab5dc                       0                 0             7.000000   \n",
       "02996f1dd2ea                       0                 0            10.000000   \n",
       "7c90be56d2ab                       0                 0             4.666667   \n",
       "\n",
       "              avg_len_words_desc  title_desc_len_ratio  \n",
       "item_id                                                 \n",
       "b912c3c6a6ad            8.285714              0.362069  \n",
       "2dac0150717d            5.857143              0.414634  \n",
       "ba83aefab5dc            5.823529              0.141414  \n",
       "02996f1dd2ea            7.333333              0.454545  \n",
       "7c90be56d2ab            6.000000              0.583333  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TF-IDF] Term Frequency Inverse Document Frequency Stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\longwind48\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Runtime: 26.78 Minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "##I added to the max_features of the description. It did not change my score much but it may be worth investigating\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=17000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('title',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "\n",
    "#Fit my vectorizer on the entire dataset instead of the training rows\n",
    "#Score improved by .0001\n",
    "vectorizer.fit(df.to_dict('records'))\n",
    "\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "\n",
    "# Drop Text Cols\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df.drop(textfeats, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2011862x1430760 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 48687245 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ready_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n",
    "\n",
    "#Ridge oof method from Faron's kernel\n",
    "#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n",
    "#It doesn't really add much to the score, but it does help lightgbm converge faster\n",
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge OOF RMSE: 0.23033890453576883\n"
     ]
    }
   ],
   "source": [
    "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
    "print('Ridge OOF RMSE: {}'.format(rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adding ridge predictions back into main dataset\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "\n",
    "df['ridge_preds'] = ridge_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503424 Rows and 1430795 Cols\n",
      "508438 Rows and 1430795 Cols\n",
      "Feature Names Length:  1430795\n"
     ]
    }
   ],
   "source": [
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "tfvocab = df.columns.tolist() + tfvocab\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "del df\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save/load matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Save and load for train-test sets\n",
    "from scipy import sparse\n",
    "\n",
    "#sparse.save_npz(\"X.npz\", X)\n",
    "X = sparse.load_npz(\"avito_ridge_2219/X.npz\")\n",
    "#sparse.save_npz(\"testing.npz\", testing)\n",
    "testing = sparse.load_npz(\"avito_ridge_2219/testing.npz\")\n",
    "\n",
    "#y.to_pickle('y.pkl')    #to save the dataframe, df to 123.pkl\n",
    "y = pd.read_pickle('avito_ridge_2219/y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz(\"ready_df.npz\", ready_df)\n",
    "#ready_df = sparse.load_npz(\"avito_ridge_2219/ready_df.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tfvocab.txt\", \"wb\") as fp:   #Pickling \n",
    "    pickle.dump(tfvocab, fp)\n",
    "\n",
    "#with open(\"tfvocab.txt\", \"rb\") as fp:   # Unpickling\n",
    "#    tfvocab = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VALID = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modeling Stage\n",
      "Light Gradient Boosting Regressor\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModeling Stage\")\n",
    "\n",
    "# del ridge_preds,vectorizer,ready_df\n",
    "# gc.collect();\n",
    "    \n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    # 'max_depth': 15,\n",
    "    'num_leaves': 270,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.016,\n",
    "    'verbose': 0\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\longwind48\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 9min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if VALID == True:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.10, random_state=2018)\n",
    "        \n",
    "    # LGBM Dataset Formatting \n",
    "    lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                    feature_name=tfvocab,\n",
    "                    categorical_feature = categorical)\n",
    "    lgvalid = lgb.Dataset(X_valid, y_valid,\n",
    "                    feature_name=tfvocab,\n",
    "                    categorical_feature = categorical)\n",
    "    del X, X_train; gc.collect()\n",
    "    \n",
    "    # Go Go Go\n",
    "    lgb_clf = lgb.train(\n",
    "        lgbm_params,\n",
    "        lgtrain,\n",
    "        num_boost_round=20000,\n",
    "        valid_sets=[lgtrain, lgvalid],\n",
    "        valid_names=['train','valid'],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    print(\"Model Evaluation Stage\")\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "    del X_valid ; gc.collect()\n",
    "\n",
    "else:\n",
    "    # LGBM Dataset Formatting \n",
    "    lgtrain = lgb.Dataset(X, y,\n",
    "                    feature_name=tfvocab,\n",
    "                    categorical_feature = categorical)\n",
    "    #del X; gc.collect()\n",
    "    # Go Go Go\n",
    "    lgb_clf = lgb.train(\n",
    "        lgbm_params,\n",
    "        lgtrain,\n",
    "        num_boost_round=1565,\n",
    "        verbose_eval=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1d76e855630>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avito_ridge_2219/vanilla_lgb.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# save model\n",
    "joblib.dump(lgb_clf, 'avito_ridge_2219/vanilla_lgb.pkl')\n",
    "# load model\n",
    "#gbm_pickle = joblib.load('lgb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Feature Importance Plot\n",
    "f, ax = plt.subplots(figsize=[7,10])\n",
    "lgb.plot_importance(lgb_clf, max_num_features=100, ax=ax)\n",
    "plt.title(\"Light GBM Feature Importance\")\n",
    "plt.savefig('feature_import.png')\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\longwind48\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:447: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Evaluation Stage\")\n",
    "lgpred = lgb_clf.predict(testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "lgsub = lgsub['deal_probability'].clip(0.0, 1.0)\n",
    "lgsub.to_csv(\"final_ridge_aggfeats-216761-.csv\",index=True,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
